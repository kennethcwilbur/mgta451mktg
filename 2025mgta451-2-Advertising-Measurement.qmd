---
title: "Advertising Measurement"
subtitle: "UCSD MGTA 451 — Marketing"
author: "Kenneth C. Wilbur"
format:
  revealjs:
    theme: default
    slide-number: c
    chalkboard: true
editor: visual 
---

## Agenda {.smaller}

-   Advertising Importance
-   Causality
-   Fundamental Problem of Causal Inference
-   Advertising Measurement
-   Correlational Advertising Measurement
-   Advertising Experiments & Quasi-experiments
-   Industry practices
-   Marketing Mix Models
-   Career considerations

# Advertising Importance

## Publisher revenue since 1950

![](images/benedictevans-adrev.png){fig-align="center" width="10.5in"}

::: aside
[source](https://www.ben-evans.com/benedictevans/2020/6/14/75-years-of-us-advertising){target="_blank"}. Appears to tally ad seller revenues, excluding ad supply chain fees.
:::

::: notes
Around 1 out of every \$100 spent in the US buys an ad
:::

## 

![](images/IARRgrowthbyformat2020-24){fig-align="center" width="9in"}

::: aside
[source](https://www.iab.com/wp-content/uploads/2025/04/IAB_PwC-Internet-Ad-Revenue-Report-Full-Year-2024.pdf){target="_blank"}
:::

::: notes
Revenue growth remains substantial
:::

## 

![](images/stlfed-adspendbyyear.png){fig-align="center" width="7.5in"}

::: notes
-   Another source shows how the numbers differ depending on whether you count money spent or money received. Most of the difference likely goes to intermediaries, e.g. agencies, DSPs, SSPs, etc
-   Differences also depend on whether your definition includes direct mail\
:::

::: aside
[source](https://www.stlouisfed.org/on-the-economy/2024/oct/rise-digital-advertising-economic-implications){target="_blank"}
:::

## 

![](images/stlfed-big4adrevbyyear.png){fig-align="center" width="7.5in"}

::: notes
:::

::: aside
[source](https://www.stlouisfed.org/on-the-economy/2024/oct/rise-digital-advertising-economic-implications){target="_blank"}
:::

## 

![](images/stlfed-big4profitpct.png){fig-align="center" width="7in"}

::: notes
Selling ads is a high-margin business for some companies, compared to other services
:::

::: aside
[source](https://www.stlouisfed.org/on-the-economy/2024/oct/rise-digital-advertising-economic-implications){target="_blank"}
:::

## 

![](images/iab-programmatic.png){fig-align="center" width="8.8in"}

::: aside
One reason ad sellers are so profitable is programmatic advertising, which uses technology to *automate* and *optimize* ad sales
:::

::: notes
The automation benefit is pretty obvious, as it speeds things up compared to manual processing. The optimization is less obvious but likely even more consequential, as algorithms can be designed to incorporate automated test-and-learn strategies using conversion data. 
:::


## 

![](images/pmax.png){fig-align="center" width="9in"}

::: aside
[1](https://support.google.com/google-ads/answer/10724817?hl=en){target="_blank"}, [2](https://blog.google/products/ads-commerce/channel-performance-reporting-coming-to-performance-max/){target="_blank"}. See also [Meta Advantage+](https://www.facebook.com/business/help/733979527611858){target="_blank"}
:::

::: notes
Google Pmax is the ultimate expression of programmatic advertising. Google will decide where, when and how to spend your money for you, then tell you how well it did. Launched in 2021; Over 1 million advertisers served by 2025. Don't sweat the details!

:::


## 

![](images/isba-supplychain-2020.png){fig-align="center" width="7.5in"}

::: aside
[source](https://www.isba.org.uk/system/files/media/documents/2023-01/ISBA%20%20PwC%20programmatic%20supply%20chain%20study%20II%20%28summary%29-%2018%20January%202023.pdf){target="_blank"}
::: 

::: notes
This is called the "ad tech tax." Programmatic advertising turns out to be pretty expensive. Let's talk about the major players. (Note: The 2022 revision shows some different numbers and makes some different, more-complicated reporting tradeoffs. The 2023 report did not offer as much public data but the report summary showed 65% of spend getting received by publishers.)
:::

## 

![](images/mobile_lumascape.png){fig-align="center" width="8.5in"}


::: aside
[source](https://lumapartners.com/lumascapes/)
:::

::: notes
Luma Partners describes itself as the leading investment bank in digital. It maps markets for major types of internet marketing, internet advertising and marketing technology. Let's talk through how some of this works.
:::


## 

![](images/ads_ds_avoid.png){fig-align="center" width="10in"}

::: aside
[source.](https://drive.google.com/file/d/126xyjFPuZAQW3EcHIa_LPKN_yc5QU3zB/view?usp=sharing){target="_blank"} Publishers sell "Opportunities to See", not "Exposures" ; see [Viewability](https://en.wikipedia.org/wiki/Media_Rating_Council#Viewability_standard){target="_blank"}
:::

::: notes
Do ads work? We spend all that money, but 45-55% of people say they try to avoid ads!
:::

## 

![](images/effective-frequency.png){fig-align="center" height="6.2in"}

::: aside
[1](https://blog.seznam.cz/en/2023/01/how-to-find-the-ideal-frequency-in-branded-campaigns/){target="_blank"}, [2](https://www.mediapost.com/publications/article/400180/advertisers-losing-consumers-to-ad-fatigue.html){target="_blank"}
:::

::: notes
One possible role of programmatic advertising is to find the right number of exposures per targeted individual; will it be configured that way? (might depend on who configures it, publisher, adtech or advertiser)
:::

## 

![](images/ads_ds_time_spending.png){fig-align="center" width="9in"}

::: aside
Consumer attention is more scarce in some media than others
:::

## 

![](images/adsalesratios2024.png){fig-align="center" width="4.5in"}

```         
    - Across sectors, typical ad-sales ratio is 2.83\%        
    - Typical public company net margin: 8-10% 
    - So modal firm could increase EBITDA 28-35% by dropping ads: (8+2.83)/8=1.35
    - Or could it? What would happen to top-line revenue?
```

::: aside
[SAI Books source](https://saibooks.com/advertising-sales-ratios/){target="_blank"}; & [Damodaran](https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datafile/margin.html){target="_blank"} as typical net margin source
:::

::: notes
-   [also relevant](https://www.rab.com/research/10014.pdf){target="_blank"}
-   If you care about profit, you should care about advertising
:::

## Toy economics of advertising {.smaller}

-   Suppose we pay \$10 to buy 1,000 digital ad OTS. Suppose 3 people click, 1 person buys.

-   Ad profit \> 0 if transaction margin \> \$10

    ```         
        - But we bought ads for 999 people who didn't buy
    ```

-   Or, ad profit \> 0 if CLV \> \$10

    ```         
        - Long-term mentality justifies increased ad budget  
    ```

-   Or, ad profit \> 0 if CLV \> \$10 *and* if the customer would not have purchased otherwise

    ```         
        - This is "incrementality"
        - But how would we know if they would have purchased otherwise?
    ```

-   Ad effects are subtle--typically, 99.5-99.9% *don't* convert--but ad profit can still be robust

    ```         
        - Ad profit depends on ad cost, conversion rate, margin ... and how we formulate our objective function
        - Exception: Search ads are atypical, you may see conversion rates like 1-5%, but incrementality questions are even more important
    ```

::: notes
How important is that incrementality point? 
:::


## 

![](images/incrementality.png){fig-align="center" width="7.5in"}



## 

![](images/ebay){fig-align="center" width="9in"}

::: aside
[1](https://www.iab.com/wp-content/uploads/2023/04/IAB-Prohaska-MA-Workshop_LA-04.04.23_FOR-DISTRIBUTION.pdf){target="_blank"}, [2](https://faculty.haas.berkeley.edu/stadelis/BNT_ECMA_rev.pdf){target="_blank"}
::: 

::: notes
- A famous study run by economists at eBay randomly buying google keyword advertising in a subset of cities
- At the time, eBay was a top-5 Google client, spending over $1 billion per year on search ads
- Clicks on paid links went to zero, but the effect was entirely replaced with clicks on organic links
- Interestingly, the eBay's "attributed sales" did not materially change. That led eBay to fire some of its staff
:::

## 

![](images/honey.png){fig-align="center" width="8in"}

::: aside
[source](https://www.youtube.com/watch?v=vc4yL3YTwWk){target="_blank"}
:::

::: notes
- Note the "17 million members" and the Paypal ownership
- Honey had two major problems. First, its pitch to consumers is literally false, as it enabled stores to pay fees to manage the coupon codes available to consumers, and thereby avoid margin erosion. Second, it was replacing influencers' affiliate codes with its own, meaning it was literally stealing affiliate marketing fees from its own partners. This is the latest installment in a long series of attribution fraud schemes, the most famous of which is called "cookie stuffing."
- One reason causality is important is that, if we assume all marketing is incremental, we will maximize our unproductive payouts and thereby decrease our advertising ROAS
:::

# Causality

Examples, fallacies and motivations

## 

![](images/margarine.svg){fig-align="center" width="9in"}

## 

-   Suppose 10 outcomes, 1000 predictors, N=100,000 obs

    ```         
    - Outcomes might include visits, sales, reviews, ...
    - Predictors might include ads, customer attributes & behaviors, device/session attributes, ...
    - You calculate 10k bivariate correlation coefficients
    ```

-   Suppose everything is noise, no true relationships

    ```         
    - The distribution of the 10,000 correlation coefficients would be Normal, tightly centered around zero
    - A 2-sided test of {corr == 0} would reject at 95% if |r|>.0062
    ```

-   We should expect 500 false positives - What is a 'false positive' exactly?

-   In general, what can we learn from a significant correlation?

    ```         
    - "These two variables likely move together." Anything more requires assumptions.
    ```

::: aside
-   [This R Script](https://drive.google.com/file/d/1NuHbXi8QNVkaJQ7QXBWiHqHfvtUaXlCC/view?usp=sharing){target="_blank"} simulates this scenario
:::

## Classic misleading correlations {.smaller}

-   "Lucky socks" and sports wins

    ```         
        - Post hoc fallacy [1] (precedence indicates causality AKA superstition)
    ```

-   Commuters carrying umbrellas and rain

    ```         
        - Forward-looking behavior
    ```

-   Kids receiving tutoring and grades

    ```         
        - Reverse causality / selection bias
    ```

-   Ice cream sales and drowning deaths

    ```         
        - Unobserved confounds
    ```

-   Correlations are measurable & usually predictive, but hard to interpret causally

    ```         
        - Correlation-based beliefs are hard to disprove and therefore sticky
        - Correlations that reinforce logical theories are especially sticky
        - Correlation-based beliefs may or may not reflect causal relationships
    ```

::: aside
\[[1](https://en.wikipedia.org/wiki/Post_hoc_ergo_propter_hoc){target="_blank"}\]
:::

## "Revenue too high alert"

![](images/BingRevenueTooHighTreatment.png){fig-align="center" height="6in"}

::: aside
[source](https://hbr.org/2017/09/the-surprising-power-of-online-experiments){target="_blank"}
:::

::: notes
This AB test triggered a "revenue too high" alert at Microsoft Bing in 2012 The treatment improved horizontal space usage and enlarged a selling argument in search ads Bing had a robust experimentation platform and culture. They run over 10k tests/year The RTH alert indicates a possible code error that might be defrauding advertisers In this case it was just a really big treatment effect, biggest ever. This change increased revenue 12% without significant harm to user experience metrics. Over \$100 million per year We can explain it ex post (HARKing) but nobody expected such a big effect
:::

## 

![](images/CvC.png){fig-align="center" width="9in"}

::: notes
Key point here: The Correlation guy is not just silly or harmless. He's weighing down the truck. & there is an opportunity cost to having him in the truck, which is that he could be pushing the truck instead.
:::

::: aside
[source](https://s3.amazonaws.com/fieldexperiments-papers2/papers/00779.pdf){target="_blank"}
:::

## 

![](images/4typesofanalytics.png){fig-align="center" width="9in"}

::: aside
-   Correlations are descriptive analytics ("facts")
-   Causality matters most for diagnostic and prescriptive analytics
-   Causality can help build predictive models, but predictive correlations may suffice
:::

# Fundamental Problem of Causal Inference

## Causal Inference

-   Suppose we have a binary "treatment" or "policy" variable $T_i$ that we can "assign" to person $i$

    ```         
    - Examples: Advertise, Serve a design, Recommend a product
    ```

-   Suppose person $i$ could have a binary potential "response" or "outcome" variable $Y_i(T_i)$

    ```         
    - Examples: Visit site, Click product, Add to Cart, Purchase, Rate, Review
    - Looks like the marketing funnel model we saw previously
    - "Treatment" terminology came from medical literature; Y could be patient outcome    
    ```

-   Important: $Y_i$ may depend fully, partially, or not at all on $T_i$, and the relationship may differ across people

    ```         
      - Person 1 may buy due to an ad; person 2 may stop due to an ad
    ```

## Why care?

-   We want to maximize profits $\Pi = \Sigma_i \pi_i(Y_i(T_i), T_i)$

-   Suppose $Y_i=1$ contributes to revenue; then $\frac{\partial \pi_i}{\partial Y_i} >0$

-   Suppose $T_i=1$ has a known cost, so $\frac{\partial \pi_i}{\partial T_i} <0$

- Effect of $T_i=1$ on $\pi_i$ is $\frac{d\pi_i}{dT_i}=\frac{\partial \pi_i}{\partial Y_i}\frac{\partial Y_i}{\partial T_i}+\frac{\partial \pi_i}{\partial T_i}$

-   We have to know $\frac{\partial Y_i}{\partial T_i}$ to optimize $T_i$ assignments

    ```         
      - Called the "treatment effect" (TE) 
    ```

-   Profits may decrease if we misallocate $T_i$

## **Fundamental Problem of Causal Inference**

-   We can only observe **either** $Y_i(T_i=1)$ **or** $Y_i(T_i=0)$, but not both, for each person $i$

    ```         
        - The case we don't observe is called the "counterfactual"
    ```

-   This is a missing-data problem that we cannot resolve. We only have one reality

    ```         
        - A major reason we build models is to compensate for missing data 
    ```

::: aside
[Rubin Causal Model](https://en.wikipedia.org/wiki/Rubin_causal_model){target="_blank"}
:::

## So what can we do?  {.smaller}

1.  Experiment. Randomize $T_i$ and estimate $\frac{\partial Y_i}{\partial T_i}$ as avg $Y_i(T_i=1)-Y_i(T_i=0)$

    ```         
     - Called the "Average Treatment Effect"
     - Creates new data; costs time, money, effort; deceptively difficult to design and then act on
    ```

2.  Use assumptions & data to estimate a "quasi-experimental" average treatment effect using archival data

    ```         
     - Requires expertise, time, effort; difficult to validate; not always possible
    ```

3.  Use correlations: Assume past treatments were assigned randomly, use past data to estimate $\frac{\partial Y_i}{\partial T_i}$

    ```         
     - Much easier than 1 or 2
     - But T is only randomly assigned when we run an experiment, so what exactly are we doing here?
     - Are we paying our agencies to distribute our ads randomly?
    ```

4.  Fuhgeddaboutit, go with the vibes, do what we feel

    ```         
     - Lots of advertisers do this
    ```

## How much does causality matter?

```         
      - How hard should we work?
```

-   Are organizational incentives aligned with profits?

-   Data thickness: How likely can we get a good estimate?

-   Organizational analytics culture: Will we act on what we learn?

-   Individual: promotion, bonus, reputation, career <br>Will credit be stolen or blame be shared?

-   Accountability: Will ex-post attributions verify findings? Will results threaten or complement rival teams/execs?

    ```         
        - Analytics culture starts at the top
    ```

# Advertising measurement

## 

- *Advertising measurement* quantifies ad delivery, exposure and outcomes to improve advertising efforts

          - Delivery: Were the ads sent as agreed? Can we verify? Did humans receive them? Were frequency caps respected?
          - Exposure: Which ads were visible? Did consumers pay attention?
          - Outcomes: Which ads generated attributable conversions? How did unattributable conversions change?
          - Counterfactuals: What would have happened without the advertising campaign?
          - Implications: How to improve our next campaign?


- Advertising measurement is expensive, so must *directly* inform firm choices, else likely to be unprofitable

          - Should we advertise? How much to budget?
          - How to split budget between advertising types and times?
          - How to split budget within advertising types and times?
          - How much to bid for which opportunities?
          - When/where/whom to target? What to say?

## What do we measure?

Most often, we measure Return on Advertising Spend (ROAS)

$\frac{\text{Revenue Attributed to Ads}}{\text{Ad Spending}}$ or $\frac{\text{Revenue Attributed to Ads}-\text{Ad Spending}}{\text{Ad Spending}}$

Increasingly, we report incremental ROAS (iROAS) if we have causal identification 

          - This is because attribution is usually correlational

We also should measure funnel-wide KPIs, e.g. brand metrics, visits, add-to-cart, sales, revenue, ...

          - We usually get economies of scope in measurement

## Diminishing returns

![](images/diminishingreturns.png){fig-align="center" width="5.5in"}

          In theory, we buy the best ad opportunities first
          So, increasing spend should lower marginal returns ("saturation")
          Marginal ROAS (ROAS) is the tangent to the curve
          Nonlinearity means that ROAS != mROAS
          We use ROAS for overall evaluation, and mROAS for budget reallocation
          We don't necessarily want to maximize ROAS or mROAS (why not?)
          The curve above can be S-shaped, but it's hard to prove empirically

::: aside
[source](https://facebookexperimental.github.io/Robyn/docs/features/){target="_blank" .smaller}
:::

::: notes
We often transform spending variables, such as by exponentiating or using logarithms, in MMM estimation to estimate saturation rates
:::

# Correlational advertising measurement

## 1. Lift Statistics

Compare conversion rates between people exposed to ads and people not exposed to ads

Usually reported as $\frac{Prob.\{Conversion|Ad\}}{Prob.\{Conversion|NoAd\}}$ or $\frac{Prob.\{Conversion|Ad\}-Prob.\{Conversion|NoAd\}}{Prob.\{Conversion|NoAd\}}$

Lift > 1 interpreted as ads are working

        This is purely correlational, as it ignores all targeting efforts
        
        Lift can be incremental if ads are allocated randomly

## 2. Regression

Get historical data on $Y_i$ and $T_i$ and run a regression

```         
       Could be across individuals, places, time, or combinations
       Most people use OLS or MMM, but Google's CausalImpact R package is also popular
```

The implicit assumption is that past ads were allocated randomly, i.e. correlation$==$causality

```         
      "Better to be vaguely right than precisely wrong"
      But are we the guy in the truck bed?
```

In truth, past ads were only random if we ran an experiment

## 3. Multi-Touch Attribution (MTA) {.smaller}

Get individual-level data on every touchpoint for every purchaser

          - Includes earned media, owned media & paid media (<--ads)
          - Often sourced from third parties
          
Choose a rule to attribute purchases to touchpoints 

          - Single-touch rules: Last-touch, first-touch
          - Multi-touch rules: Fractional credit, Shapley
          Historically, Last-touch was popular

MTA algorithm searches for touchpoint parameters that best-fit the conversion data given the rule

          - Credit then informs future budget allocations
          - MTA is designed to maximize attributions
          - MTA assumes advertising is the *sole* driver of conversions

MTA is mostly dead due to privacy and platform reporting changes 

          - Zombie MTA lives on, despite signal loss


## Strongest args for corr(ad,sales)

Corr(ad,sales) should contain signal

```         
      - If ads cause sales, then corr(ad,sales)>0 (probably) (we assume)
```

Some products/channels just don't sell without ads

```         
      - E.g., Direct response TV ads for 1-800 phone numbers
      - Career professionals say advertised phone #s get 0 calls without TV ads, so we know the counterfactual
      - Then they get 1-5 calls per 1k viewers, lasting up to ~30 minutes
      - What are some digital analogues to this?
```

However, this argument gets pushed too far

```         
      - For example, when search advertisers disregard organic link clicks when calculating search ad click profits
      - Notice the converse: corr(ad,sales)>0 does not imply a causal effect of ads on sales
```

## Problem 1 with corr(ad,sales)

Advertisers try to optimize ad campaign decisions

```         
        E.g. surfboards in coastal cities, not landlocked cities
```

If ad optimization increases ad response, then corr(ad,sales) will confound actual ad effect with ad optimization effect

```         
        More ads in san diego, more surfboard sales in san diego. But would we have 0 sales in SD without ads?

        Corr(ad,sales) usually overestimates the causal effect, encourages overadvertising
```

Many, many firms basically do this

```         
        It's ironic when firms that don't run experiments implicitly assume that past ads were randomized 
```

## Problem 2 with corr(ad,sales)

-   How do most advertisers set ad budgets? Historically, the top 2 ways were:

1.  Percentage of sales method, e.g. 3% or 6%

          - That's why ads:sales ratios are so often measured, for benchmarking

2.  Competitive parity

3.  ...others...

Do you see the problem here?

![](images/circularity.png){fig-align="right" width="3in"}

## Problem 3 with corr(ad,sales)

-   Leaves marketers powerless vs ~~big~~ colossal ad platforms

-   Platforms withhold data and obfuscate algorithms

    ```         
        - How many ad placements are incremental?
        - How many ad placements target likely converters?
        - How can advertisers react to adversarial ad pricing?
        - How can advertisers evaluate brand safety, targeting, context?
    ```

-   Have ad platforms ever left ad budget unspent?

    ```         
        - Would you, if you were them?
        - If not, why not? What does that imply about incrementality?
    ```

-   To balance platform power, you have to know your ad profits & vote with your feet

## U.S. v Google (2024, search case)

![](images/usvgoogle2024.png){fig-align="center" width="10in"}

::: aside
[source, pgs 76-101](https://files.lbr.cloud/public/2024-08/045110819896.pdf){target="_blank"}
:::

::: notes
-   This was written by a federal judge, an impartial observer who heard mountains of evidence on both sides
-   Note that he quotes internal google documents extensively
-   Was it legal that google tried to hide price increases from advertisers? Arguably yes, google is supposed to maximize google profits, though you could argue the practice was unethical or too short-term-focused. The case was about whether google monopolized the search engine market by excluding competitors, not whether google was supposed to be transparent about why auction prices increased. This point was simply a fact in the case that the government used to counter google's claim that it could not control ad auction prices, and therefore did not have monopolist pricing power.
:::

## Does Corr(ad,sales) work?

![](images/closeenough.png){fig-align="center" width="8in"}

::: aside
[source](https://arxiv.org/abs/2201.07055){target="_blank"}
:::

## 

![](images/gmz_descstats.png){fig-align="center" width="10in"}

## 

![](images/gmzfig2-4.png){fig-align="center" width="10in"}

## 

![](images/gmz2023_results.png){fig-align="center" width="10in"}


## Why are some teams OK with corr(ad,sales)?

1.  Some worry that if ads go to zero -\> sales go to zero

    ```         
     - For small firms or new products, without other marketing channels, this may be good logic
     - Downside of lost sales may exceed downside of foregone profits
     - However, premise implies deeper problems, i.e. need to diversify marketing efforts
     - Plus, we can run experiments without setting ads to zero, e.g. weight tests
    ```

2.  Some firms assume that correlations indicate direction of causal results

    ```         
     - The guy in the truck bed is pushing forwards right?
     - Biased estimates might lead to unbiased decisions (key word: "might")
     - But direction is only part of the picture; what about effect size?
    ```

## Why are some teams OK with corr(ad,sales)?

3.  CFO and CMO negotiate ad budget

    ```         
     - CFO asks for proof that ads work
     - CMO asks ad agencies, platforms & marketing team for proof
     - CMO sends proof to CFO ; We all carry on
     - Should adFX team report to CFO or CMO?
     ```

4.  Few rigorous analytics cultures or ex-post checks

    ```         
     - In some cultures, ex-post checks may threaten bonuses, turf; may get personal 
    ```

5.  Estimating causal effects of ads is not always easy

    ```         
     - Many firms lack expertise, discipline, execution skill
     - Ad/sales tests may be statistically inconclusive, especially if small
     - Tests may be designed without subsequent action in mind, then fail to inform future decisions ("science fair projects")
    ```

## Why are some teams OK with corr(ad,sales)?

6.  Platforms often provide correlational ad/sales estimates

    ```         
     - Which is larger, correlational or experimental ad effect estimates?
     - Which one might many client marketers prefer?
     - Platform estimates are typically "black box" without neutral auditors
     - Sometimes platforms respond to marketing clients' demand for good numbers
     - "Nobody ever got fired for buying [famous platform brand here]"
    ```

7.  Historically, agencies usually estimated RoAS

    ```         
     - Agency compensation usually relies on spending, not incremental sales
     - Principal/agent problems are common
     - Many marketing executives start at ad agencies
     - "Advertising attribution" is all about maximizing credit to ads
     - These days, more marketers have in-house agencies, and split work
    ```


## 

![](images/2024admeasurementtrends.png){fig-align="center" width="10in"}

::: aside
[source](https://www.adexchanger.com/marketers/the-ad-measurement-trends-that-reshaped-online-advertising-this-year/){target="_blank"}
:::

::: notes
Incrementality & MMM were trends 1 & 2; the only other was e-commerce metric proliferation 
:::

## 

![](images/gtrends_adexp.png){fig-align="center" width="9in"}

          - I believe we're a few years into a generational shift
          - However, corr(ad,sales) is not going away
          - Union(correlations, experiments) should exceed either alone



# Causal advertising measurement

## Ad Experiments: Common Designs {.scrollable .smaller}

1.  Randomly assign ads to customer groups on a platform; measure sales in each group

        - Pros: AB testing is easy to understand, rules out alternate explanations
        - Cons: Can we trust the platform's "black box"? Will we get the data and all available insights?  
        
2.  Randomize messages within a campaign

3.  Randomize budget across times, places ("Geo tests")

4.  Randomize budgets, bids, or consumer segments

5.  Randomize budget over platforms, publishers, behavioral targets, contexts

          - Experimental design describes how we create data to enable treatment/control comparisons. Experimental data are amenable to any number of models or statistical analyses.
          - Causal identification is a property of the data, not the model


## Experimental necessary conditions {.smaller}

1.  Stable Unit Treatment Value Assumption (SUTVA)

    ```         
     - Treatments do not vary across units within a treatment group
     - One unit's treatment does not change other units' potential outcomes, i.e. treatments in one group do not affect outcomes in another group
     - May be violated when treated units interact on a platform
     - E.g., successful ad campaign could deplete inventory, leading to periods of product nonavailability, thereby changing other treated consumers' outcomes
     - Violations called "interference"; remedies usually start with cluster randomization
    ```

2.  Observability

    ```         
     - Non-attrition, i.e. unit outcomes remain observable
    ```

3.  Compliance

    ```         
     - Treatments assigned are treatments received
     - We have partial remedies when noncompliance is directly observed
    ```

4.  Statistical Independence

    ```         
     - Random assignment of treatments to units
    ```

::: aside
-   See [List (2025)](https://s3.amazonaws.com/fieldexperiments-papers2/papers/00792.pdf){target="_blank"} for much more
:::

## Muy importante {.scrollable}

Before you kick off your test ...

```         
    - Run A:A test before your first A:B test. Validate the infrastructure before you rely on the result. A:A test can fail for numerous reasons
    
    - Can we agree on the opportunity cost of the experiment? "Priors"
    
    - How will we act on the (uncertain) findings? Have to decide before we design. We don't want "science fair projects"
    
    - Simple example: Suppose we estimate iROAS at 1.5 with c.i. [1.45, 1.55]. Or, suppose we estimate RoAS at 1.5 with c.i. [-1.1, 4.1]. What actions would follow each?
    
```


## Productive experiments ...   {.smaller}         

- serve customer interests

          - Working against customers drives customers away

- live within theoretical frameworks

          - Science requires hypotheses if we want to learn from tests
          - Theoretical frameworks offer mechanisms and solutions

- test quantifiable hypotheses

          - Choose test size & statistical power based on hypothesis

- analyze all relevant customer metrics

          - Test positive & negative metrics, e.g. conversions & bounce rates
          - Test SR & LR metrics, e.g. trial & repurchase
          - Classic example: Pop-ups seeking email subscriptions

- acknowledge possible interactions between variables

          - E.g. price advertising effects will always depend on the price   
          
::: aside
[source](https://drive.google.com/file/d/1fcY6tdB01Dlr5UXLNFXyjCRJyvHOxXs5/view?usp=sharing){target="_blank"}
:::


## Quasi-experiments Vocab {.smaller}

**Model**: Mathematical relationship between variables that simplifies reality, eg y=xb+e

**Identification strategy**: Set of assumptions that isolate a causal effect $\frac{\partial Y_i}{\partial T_i}$ from other factors that may influence $Y_i$

```         
      - A strategy to compare apples with apples, not apples with oranges
```

We say we "identify" the causal effect if we have an identification strategy that reliably distinguishes $\frac{\partial Y_i}{\partial T_i}$ from possibly correlated unobserved factors that also influence $Y_i$

If you estimate a model without an identification strategy, you should interpret the results as correlational

      - This is widely, widely misunderstood

You can have an identification strategy without a model, e.g. <br>$\text{avg\{Y_i(T_i=1)-Y_i(T_i=0)\}}$

Usually you want both. Models reduce uncertainty by controlling for covariates and enable counterfactual predictions

## Ad/sales: Quasi-experiments

Goal: Find a "natural experiment" in which $T_i$ is "as if" randomly assigned, to identify $\frac{\partial T_i}{\partial Y_i}$

Possibilities:

          - Firm started, stopped or pulsed advertising without changing other variables, especially when staggered across times or geos
          - Competitor starts, stops or pulses advertising
          - Discontinuous changes in ad copy 
          - Exogenous changes in ad prices, availability or targeting (e.g., elections)
          - Exogenous changes in addressable market, website visitors, or other factors

## DFS TV ad effects on Google Search

![](images/dfs_search_2015.png){fig-align="center" width="9in"}

::: aside
[source](https://drive.google.com/file/d/175yrLDY-W15TgBtumPa11WnwMwCeUmSF/view){target="_blank"} ; DFS = Daily Fantasy Sports
:::

## Ad/sales: Quasi-experiments (2) {.smaller}

Or, construct a "quasi-control group"

-   Customers or markets with similar demand trends where the firm never advertised

-   Competitors or complementors with similar demand trends that don't advertise

Helpful identification strategies: Difference in differences, Synthetic control, Regression discontinuity, Matching, Instrumental variables

In each case, we try to predict our missing counterfactual data, then estimate the causal effect as observed outcomes minus predicted outcomes


::: aside
[Comprehensive guide](https://psantanna.com/did-resources/){target="_blank"} to difference-in-difference methods
:::

## Experiments vs. Quasi-experiments

- Experimentalists and quasi-experimentalists have different beliefs, cultures & training

- Generally speaking, quasi-experiments : 

        - Always depend on untestable assumptions (as do experiments)
        - Are bigger, faster & cheaper than experiments when valid
        - Will lead us astray when not valid
        - Are easy to apply without validity
        - Fall between difficult and impossible to validate. We can evaluate treatment & control similarity on observables, but not on unobservables
      
- Experiments & quasi-experiments should be "yes-and," <br>not "either-or"

          - Strong preferences are common

# Industry practices

## Who tests the most?

![](images/2023googletesting.png){fig-align="center" width="10.5in"}

::: aside
[source](https://www.google.com/intl/en_us/search/howsearchworks/how-search-works/rigorous-testing/){target="_blank"}
:::

## {.smaller}

“To invent you have to experiment, and if you know in advance that it’s going to work, it’s not an experiment.” --Bezos, Amazon 

“In a culture that prioritizes curiosity over innate brilliance, ‘the learn‑it‑all does better than the know‑it‑all.’” --Nadella, Microsoft 

“We ship imperfect products but we have a very tight feedback loop and we learn and we get better.” --Altman, OpenAI

“You do a lot of experimentation, an A/B test to figure out what you want to do.” --Chesky, Airbnb 

“The only way to get there is through super, super aggressive experimentation.” --Khosrowshahi, Uber 

“Create an A/B testing infrastructure.” Huffman, on his top priority as Reddit CEO 

::: aside
- This list could be much longer.
- Why do you think there is such a consensus among digital-first business leaders?
- How do you think these firms measure their advertising?
- What does this imply for your career?
:::



## Advertising experiment frequency

![](images/runge2020HBR.png){fig-align="center" width="10in"}

::: aside
[1](https://hbr.org/2020/10/marketers-underuse-ad-experiments-thats-a-big-mistake){target="_blank"}, [2](https://drive.google.com/file/d/1Xetwnrv6dLyhXKlTK0vYaZ5yEnB6K8iY/view?usp=sharing){target="_blank"}
:::

## Advertising experiment effectiveness
 
            - Authors describe how companies' experimental practices relate to last-click conversion/ad$ metrics, and incremental conversions/ad$ in Meta experiments
            
![](images/runge2020HBR2.png){fig-align="center" width="10.5in"}

::: aside
[source.](https://drive.google.com/file/d/1Xetwnrv6dLyhXKlTK0vYaZ5yEnB6K8iY/view?usp=sharing){target="_blank"} Results are correlational; experimentation is not randomly assigned
:::
            
::: notes
These are "heat maps" which visualize 3-dimensional distributions in 2D spaces. The X axis measures each company's number of experiments run in the most recent year. The Y axis measures each company's lifetime number of experiments run. The Z axis measures last-click conversions per ad dollar (on the left) and incremental conversions per dollar in experiments (on the right). What you see is that companies with deep experimental practices, in terms of more historical experiments and more recent experiments, tend to get much better results for per ad dollar spent. This is strongly suggestive descriptive evidence.
:::
            

## 

![](images/kantar2025-0.png){fig-align="center" width="10in"}

::: aside
[source](https://drive.google.com/file/d/1AsafqvDMIJe1BvQ7hBXB1iQiFa4uNJ5N/view?usp=sharing){target="_blank"}
:::

::: notes
Previous data was pre-2020, what about more recently?
:::

## 

![](images/kantar2025-1.png){fig-align="center" width="10in"}

::: aside
[source](https://drive.google.com/file/d/1AsafqvDMIJe1BvQ7hBXB1iQiFa4uNJ5N/view?usp=sharing){target="_blank"}
:::

## 

![](images/kantar2025-2.png){fig-align="center" width="7in"}

::: aside
[source](https://drive.google.com/file/d/1AsafqvDMIJe1BvQ7hBXB1iQiFa4uNJ5N/view?usp=sharing){target="_blank"}
:::

## 

![](images/kantar2025-3.png){fig-align="center" width="6.5in"}

::: aside
[source](https://drive.google.com/file/d/1AsafqvDMIJe1BvQ7hBXB1iQiFa4uNJ5N/view?usp=sharing){target="_blank"}
:::

## 

![](images/kantar2025-4){fig-align="center" width="=10in"}

::: aside
[source](https://drive.google.com/file/d/1AsafqvDMIJe1BvQ7hBXB1iQiFa4uNJ5N/view?usp=sharing){target="_blank"}
:::

## 

![](images/kantar2025-5.png){fig-align="center" width="7.5in"}

::: aside
[source](https://drive.google.com/file/d/1AsafqvDMIJe1BvQ7hBXB1iQiFa4uNJ5N/view?usp=sharing){target="_blank"}
:::            
            

# Marketing Mix Models

## Marketing Mix Models {.smaller}

-   The "marketing mix" consists of the 4 P's

          - E.g. product line, length and features; price & promotions; advertising, PR, social media and other marcoms; retail distribution breadth, intensity and quality

-   A "marketing mix model" (MMM) relates sales to marketing mix variables 

    ```         
        - Idea goes back to the 1950s
        - E.g., suppose we increase price & ads at the same time
        - Or, suppose ads increased demand, and then inventory-based systems raised prices
        - When possible, MMM should include competitor variables also
    ```

-   A "media mix model" (mMM) relates sales to ads/marcom channels 

    ```         
        - MMM and mMM share many models and techniques
    ```

-   MMM goal is to evaluate past marketing efforts, and better inform future efforts

::: aside
Pioneering works: [Magee (1953)](https://pubsonline.informs.org/doi/abs/10.1287/opre.1.2.64){target="_blank"}, [Weinberg (1956)](https://pubsonline.informs.org/doi/10.1287/opre.4.2.152){target="_blank"}, [Vidale and Wolfe (1957)](https://pubsonline.informs.org/doi/abs/10.1287/opre.5.3.370){target="_blank"}, [Lambin (1972)](https://journals.sagepub.com/doi/10.1177/002224377200900201){target="_blank"}, [Little (1972)](https://pubsonline.informs.org/doi/abs/10.1287/opre.23.4.628){target="_blank"}
:::



## MMM components {.smaller}

MMM analyzes aggregate data, usually 3-5 years of weekly or monthly intervals, usually across a panel of geos (e.g., states, counties, CMSAs)

          - Aggregate data are privacy-compliant & often do not require platform participation; helps explain MMM comeback

Predictors include ad spending/exposures by ad type; outcomes measure sales, volume or revenue

          - Spending data is nearly always available. Exposures can be better if measured accurately

MMM usually controls for (a) trends, (b) seasonality, (c) macroeconomic factors, (d) known demand shifters, (e) saturation, (f) carryover

          - Sometimes accounts for interactions between push/pull channels as well

Analysts use data to select model specifications under these constraints. Estimation can be frequentist, Bayesian, ML-based

Outputs include ad elasticities, ROAS measures, and budget reallocation

MMM parameters can be interpreted causally if and only if adspend data are generated with a randomization strategy built in

## MMM Considerations {.smaller}

-   Data availability, accuracy, granularity and refresh rate are all critical

-   MMM requires sufficient variation in marketing predictors, else it cannot estimate coefficients

-   "Model uncertainty" : Results can be strongly sensitive to modeling choices, so we usually evaluate multiple models 

- MMM results are correlational without experiments or quasi-experimental identification 

          - Correlations can be unstable; Bayesian estimation can help regularize
          - MMM results can be causal if you induce exogenous variation in ad spending 

-   For much more, see this [MSI White Paper](https://github.com/kennethcwilbur/website/raw/master/MSI-MMM-Blue-Ribbon-Panel-Report-Updated.pdf){target="_blank"} 


## Open-source Frameworks

- Meta [Robyn](https://facebookexperimental.github.io/Robyn/){target="_blank"} (2024). Excellent [training course](){target="_blank"}

          - Cool features: Causal estimate calibration, Set your own objective criteria, Smart multicollinearity handling
          
- Google [Meridian](https://developers.google.com/meridian){target="_blank"} (2025). Excellent [self-starter guide](https://www.thinkwithgoogle.com/_qs/documents/18498/Meridian_Playbook_1s4EUSU.pdf){target="_blank"}

          - Cool features: Bayesian implementation, Hierarchical geo-level modeling, reach/frequency distinctions
          - Robyn & Meridian both include budget-reallocation modules

Others: [PyMC-Marketing](https://www.pymc-marketing.io/en/stable/guide/mmm/mmm_intro.html){target="_blank"}, [mmm_stan](https://github.com/sibylhe/mmm_stan){target="_blank"}, [bayesmmm](https://github.com/nrhodes1451/bayesmmm){target="_blank"}, [BayesianMMM](https://github.com/leopoldavezac/BayesianMMM){target="_blank"}

Also relevant: an [MMM data simulator](https://facebookexperimental.github.io/siMMMulator/){target="_blank"}


# Career considerations

## 

![](images/li1.png){fig-align="center" width="4in"}

![](images/li2.png){fig-align="center" width="10in"}

::: notes
It's interesting that comments mainly came from those with little to lose
:::

## Ken's take {.smaller}

-   Adopting incremental methods is a resume headline & interesting challenge

    ```         
        - Team may have a narrow view of experiments or how to act on them
        - Understanding that view is the first step toward addressing it
        - Reach up the org chart, you will need leadership onboard
    ```

-   Correlational + Incremental \> Either alone

    ```         
        - What incrementality might be valuable? What's our hardest challenge?
        - What quasi-experimental measurement opportunities exist?
        - Can we estimate the relationship between incremental and correlational KPIs?
    ```

-   Going-dark design

    ```         
        - Turn off ads in (truly) random 5% of places/times; nominally free, though arguably costly if it foregoes some sales 
        - How do going-dark sales data compare to correlational model's predicted sales?
        - Can we improve the model & motivate more informative experiments?
    ```

-   If structural incentives misalign, consider a new role

    ```         
        - You can't reform a culture without being in the right position 
        - Life is short, do something meaningful          
    ```

## Takeaways

-   Fundamental Problem of Causal Inference: <br>We can't observe all data needed to optimize actions. <br>This is a missing-data problem, not a modeling problem.

    ```         
      - Common remedies: Experiments, Quasi-experiments, Correlations, Triangulate; Ignore
    ```

-   Experiments are the gold standard, but are costly and challenging to design, implement and act on

-   Ad effects are subtle but that does not imply unprofitable. Measurement is required

![](images/recap.png){fig-align="right" width="2in"}

## Going deeper {.smaller}

-   [Retail Media ROAS Demystified: A Guide to Understanding Your Brand's ROAS](https://drive.google.com/file/d/1Mj3-WCW9CSFDDUdT6s4rEo_bFB7y8Wcx/view?usp=sharing){target="_blank"} Shows how some common modeling and measurement change correlational ROAS measurements

-   [Inferno: A Guide to Field Experiments in Online Display Advertising](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3581396){target="_blank"}: Covers frequent problems in online advertising experiments

-   [Inefficiencies in Digital Advertising Markets](https://arxiv.org/abs/1912.09012){target="_blank"}: Discusses iRoAS estimation challenges and remedies; also, principal/agent problems, adblocking and ad fraud

-   [Your MMM is Broken](https://arxiv.org/abs/2408.07678){target="_blank"}: Smart discussion of key MMM assumptions

-   [The Power of Experiments](https://direct.mit.edu/books/book/5468/The-Power-of-ExperimentsDecision-Making-in-a-Data){target="_blank"}: Goes deep on digital test-and-learn considerations

-   [New Developments in Experimental Design and Analysis (2024)](https://www.youtube.com/watch?v=I6GyDWh8kfw){target="_blank"} by Athey & Imbens

![](images/takingoff.png){fig-align="right" width="2in"}
