---
title: "Causality "
subtitle: "UCSD MGTA 451-Marketing"
author: "Kenneth C. Wilbur"
output: html_document
format: 
  revealjs:
    auto-stretch: false
    theme: default
slide-number: c 
editor_options: 
  chunk_output_type: inline 
---

##  

![](images/margarine.svg){fig-align="center" style="height: 100vh"}


## 

- Suppose 10 outcome variables (visits, sales, reviews, ...), 1000 potential predictors (customer attributes, ...), N=100,000 observations

- Suppose everything is noise, no true relationships

          - The distribution of the 10,000 correlation coefficients would be Normal, tightly centered around zero

          - A 2-sided test of {corr != 0} would reject at 95% if |r|>.0062

- We should expect 500 false positives 

- In general, what can we learn from a significant correlation?

::: aside
- [This R Script](https://drive.google.com/file/d/1NuHbXi8QNVkaJQ7QXBWiHqHfvtUaXlCC/view?usp=sharing){target="_blank"} simulates this scenario
:::

## 

![](images/BingRevenueTooHighTreatment.png){fig-align="center" style="height: 7in"}

::: aside
[source](https://www.amazon.com/Power-Experiments-Decision-Making-Understanding/dp/0544113966){target="_blank"}
:::
 
::: notes
This AB test triggered a "revenue too high" alert at Microsoft Bing in 2012
The treatment improved horizontal space usage and enlarged a selling argument in search ads
Bing had a robust experimentation platform and culture. They run over 10k tests/year
The RTH alert indicates a possible code error that might be defrauding advertisers
In this case it was just a really big treatment effect, biggest ever.
This change increased revenue 12% without significant harm to user experience metrics. Over $100 million per year
We can explain it ex post (HARKing) but nobody expected such a big effect 
:::
 
## 

![](images/CvC.png){fig-align="center" style="height:90vh"}
 
::: notes
Key point here: The Correlation guy is not just silly or harmless. 
He's weighing down the truck.
There is an opportunity cost to having him in the truck, which is that he could 
be pushing the truck instead.
:::
 
::: aside
[Source](https://s3.amazonaws.com/fieldexperiments-papers2/papers/00779.pdf)
:::
 
# Agenda

- Causality
- Experiments
- Quasi-experiments
- Correlations

## 

![](images/4typesofanalytics.png){fig-align="center" style="height: 90vh"}

::: aside
-   Causality matters for diagnostic and prescriptive analytics, not descriptive or predictive
:::

## Causal Inference

-   Suppose we have a binary "treatment" or "policy" variable $T_i$ that we can "assign" to person $i$

    ```         
        - Examples: Advertise, Serve a design, Recommend, Give coupon
    ```

-   Suppose person $i$ could have a binary potential "response" variable $Y_i(T_i)$

    ```         
        - Examples: Visit site, Add to Cart, Click product, Redeem
    ```

- Important: $Y_i$ may depend fully, partially, or not at all on $T_i$, and the dependence may be different for different people


## Why care?

-   We want to maximize profits $\pi_i(Y_i(T_i), T_i)$ 

- Suppose $Y_i=1$ contributes to revenue; then $\frac{\partial \pi_i}{\partial Y_i} >0$ 

- Suppose $T_i=1$ is costly; then $\frac{\partial \pi_i}{\partial T_i}=\frac{\partial \pi_i}{\partial Y_i}\frac{\partial Y_i}{\partial T_i}+\frac{\partial \pi_i}{\partial T_i}$

- We have to know $\frac{\partial Y_i}{\partial T_i}$ to optimize $T_i$ assignments

        - Called the "treatment effect" (TE) 

- Profits may decrease if we misallocate $T_i$

## **Fundamental Problem of Causal Inference**

-   We can only observe **either** $Y_i(T_i=1)$ **or** $Y_i(T_i=0)$, but not both, for each person $i$

- This is a missing-data problem that we cannot resolve. We only have one reality

## So what can we do? {.smaller}

1. Experiment. Randomize $T_i$ and estimate $\frac{\partial Y_i}{\partial T_i}$ as avg  $Y_i(T_i=1)-Y_i(T_i=0)$

        - Creates new data; costs time, money, attention; deceptively difficult to design and then act on

2. Use assumptions & data to estimate a "quasi-experimental" average treatment effect using archival data

        - Requires expertise, time, attention; difficult to validate; not always possible

3. Use correlations: Assume past treatments were assigned randomly, use past data to estimate $\frac{\partial Y_i}{\partial T_i}$

        - Easiest approach; but likely wrong, bc T is only randomly assigned when we run an experiment

4. Fuhgeddaboutit, go with the vibes

## Does causality matter?

- Organizational returns or costs of getting it right?

- Data thickness: How likely can we get a good estimate?

- Will ex-post attributions verify findings? 

- Accountability: Will credit be stolen? Will blame be shared? Will results threaten or complement rival teams/execs?

- How does empirical approach fit with organizational analytics culture?

- Individual promotion or bonus considerations?

- Individual reputation or career consequences?

        
## Ad/sales example: Experiment {.scrollable}

1. Randomly assign ads to customers on a platform; measure sales, e.g. [Link](https://www.facebook.com/business/learn/lessons/how-to-make-your-ads-more-effective){target="_blank"}

        - Pros: AB testing is easy to understand, easy to implement, easy to validate
        
        - Cons: Can we trust the platform's "black box"? Will we get the data and all available insights? Could platform knowledge affect future ad costs?
 
2. Randomly assign messages within a campaign 
      
3. Randomly choose times, places, segments or combinations; compare treated times to controls
        
4. Randomize over budgets and bids

5. Randomly choose platforms, publishers, behavioral targets, etc., to compare RoAS across options


## Ad/sales example: Experiment (2) {.scrollable}

Key issues for any experimental design: 
        
        - ALWAYS run an A:A test first. Validate the infrastructure before you trust a result
        
        - Can we agree on the opportunity cost of the experiment? 
        
        - How will we act on the (uncertain) findings? Have to decide before we design
        
        - Example: Suppose we estimate RoAS at 0.1. Or, suppose we estimate RoAS at 0.1, but we have a 95% confidence interval of [0.05, 0.15]. What will we do with this information?
        

## Quasi-experiments vocab {.smaller }

**Model**: Mathematical relationship between variables that simplifies reality

**Identification strategy**: Set of assumptions that enable isolate a causal effect $\frac{\partial Y_i}{\partial T_i}$ from other factors that may influence $Y_i$. Also called "incrementality" in ad/sales context

We say we "identify" the causal effect if we have an identification strategy that isolates $\frac{\partial Y_i}{\partial T_i}$ from possibly correlated unobserved factors that also influence $Y_i$

Estimating a model with an identification strategy offers causal interpretation; <br>otherwise the results are correlational

You can have an identification strategy without a complicated model, e.g. avg  $Y_i(T_i=1)-Y_i(T_i=0)$

Usually you want both. Models help with quantifying uncertainty and estimating treatment effects by controlling for relevant observables. We'll focus on identification strategies today


## 2. Ad/sales: Quasi-experiments

Goal: Find a "natural experiment" in which $T_i$ is "as if" randomly assigned

Possibilities:

          - Firm starts advertising without changing other variables
          - Firm stops advertising without changing other variables
          - Big pulse in the ad budget in a random time, place or medium
          - Competitor starts advertising
          - Competitor stops advertising
          - Big change in ad price for exogenous reasons that don't affect Y (e.g., election)
          - Relevant PR or news events

## 

![](images/dfs_search_2015.png){fig-align="center" width=10in}          


## Ad/sales: Quasi-experiments (2){.scrollable .smaller}

Or, construct a "control group"

- Customers or markets with similar demand trends where the firm never advertised

- Competitors or complementors with similar demand trends that don't advertise

Techniques like "difference in differences," "synthetic control," "regression discontinuity," "matching," and "instrumental variables" can help

In each case, we try to predict our missing counterfactual data, then estimate the causal effect as observed outcomes minus predicted outcomes


## 3. Ad/sales example: Correlational

This is what most firms did historically, but less every year

Just get historical data on $Y_i$ and $T_i$ and run a regression

           Most people use OLS, but Google's CausalImpact R package is also popular

The implicit assumption is either that there are no unobservables that influence both $Y_i$ and $T_i$ ; or that we are OK with a correlation

          "Better to be vaguely right than precisely wrong"
          
## 3. Problem 1 with ad/sales correlations

Advertisers try hard to maximize ad effects

They select messages, media, times, target audiences, places (etc.) to get the biggest impact possible

Often, they use statistical learning models that mechanistically try to maximize correlations with sales

So, if you assume that past $T_i$ was random, it's like assuming that the advertising team is incompetent

This type of selection/treatment problem is common in marketing

## 3. Problem 2 with ad/sales correlations

Suppose you wisely advertise only in places where you expect a big response

            E.g. you advertise surfboards in coastal cities with big waves, but not in landlocked cities

You'll get a big correlation between ads and sales, partly because of differences in addressable markets

So then you'll overestimate the effect of ads on sales

Yet many, many firms basically do this

## Why are firms OK with ad/sales correlations?

Most commonly: CFO and CMO negotiate over the ad budget

CFO asks for proof that ads work

CMO asks agencies, platforms & marketing team for quantifiable proof

CMO provides the proof to CFO ; We all carry on

          - Things may change if ad effects team reports to CFO

No rigorous analytics culture or backward-looking checks. Of course, there are exceptions

Also, estimating causal effects of ads can be pretty difficult


##

![](images/adsalesratios2024.png){fig-align="center" width=8in}

::: aside
[source](https://saibooks.com/advertising-sales-ratios/){target="_blank"}
:::
::: notes
[also relevant](https://www.rab.com/research/10014.pdf){target="_blank"}
:::

## 

![](images/hawksem.png){fig-align="center" width=7in}

::: aside
- Maybe the right question is what % of sales to spend on experiments to get more sales
:::


## Takeaways

- Fundamental Problem of Causal Inference: We can't observe all relevant data to optimize actions

- Remedies: Experiments, Quasi-experiments, Correlations

- Experiments are the gold standard, but are costly and difficult to design, implement and act on

![](images/recap.png){fig-align="right" width="2in"}

## Going deeper

- [What is Incrementality? And How Do We Measure it in 2024?](https://www.incrmntal.com/resources/how-do-we-measure-incrementality){target="_blank"}

-  [The Power of Experiments](https://www.amazon.com/Power-Experiments-Decision-Making-Understanding/dp/0544113966){target="_blank"}: Goes deep on digital test-and-learn considerations

- [Mostly Harmless Econometrics](https://www.amazon.com/Mostly-Harmless-Econometrics-Empiricists-Companion/dp/0691120358){target="_blank"}: Guide to some Quasi-experimental techniques

- [Inferno: A Guide to Field Experiments in Online Display Advertising](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3581396){target="_blank"}: Covers frequent problems in online advertising experiments

-   [Inefficiencies in Digital Advertising Markets](https://arxiv.org/abs/1912.09012){target="_blank"}: Discusses RoAS estimation challenges in digital settings and available remedies

![](images/takingoff.png){fig-align="right" width="2in"}
